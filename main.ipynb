{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import time \n",
    "\n",
    "from snnlib.spiking_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pickle_file = open(\"./data/CNN_train_data/labels.pickle\", \"rb\")\n",
    "train_data_pickle_file = open(\"./data/CNN_train_data/train_data.pickle\", \"rb\")\n",
    "\n",
    "labels = pickle.load(labels_pickle_file)\n",
    "train_data = pickle.load(train_data_pickle_file)\n",
    "\n",
    "snn = SCNN()\n",
    "snn.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(snn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ebf347ab672a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "for real_epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(5):\n",
    "        for i, (images, labels) in enumerate(zip(train_data, labels)):\n",
    "            print(images.shape)\n",
    "\n",
    "            snn.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for j in range(images.shape[0]):\n",
    "                img0 = images[j, 0, :, :].numpy()\n",
    "                rows, cols = img0.shape\n",
    "                for k in range(10):\n",
    "                    images2[j * 2, k, :, :] = torch.from_numpy(img0)\n",
    "                    labels2[j * 2] = labels[j]\n",
    "\n",
    "            images2 = images2.float().to(device)\n",
    "\n",
    "            outputs = snn(images2)\n",
    "            labels_ = torch.zeros(batch_size * 2, 20).scatter_(1, labels2.view(-1, 1), 1)\n",
    "            loss = criterion(outputs.cpu(), labels_)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Real_Epoch [%d/%d], Epoch [%d/%d], Step [%d/%d], Loss: %.5f'\n",
    "                        %( real_epoch, num_epochs, epoch, 5, i+1, len(train_dataset)//batch_size, running_loss))\n",
    "                running_loss = 0\n",
    "                print('Time elasped:', time.time() - start_time)\n",
    "\n",
    "    # ================================== Test ==============================\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = lr_scheduler(optimizer, epoch, learning_rate, 40)\n",
    "    cm = np.zeros((20, 20), dtype=np.int32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "            images2 = torch.empty((images.shape[0] * 2, 10, images.shape[2], images.shape[3]))\n",
    "            labels2 = torch.empty((images.shape[0] * 2), dtype=torch.int64)\n",
    "            for j in range(images.shape[0]):\n",
    "                img0 = images[j, 0, :, :].numpy()\n",
    "                rows, cols = img0.shape\n",
    "                theta1 = 0\n",
    "                theta2 = 360\n",
    "                for k in range(10):\n",
    "                    if k == 0 or k == 9:\n",
    "                        images2[j * 2, k, :, :] = torch.from_numpy(img0)\n",
    "                    else:\n",
    "\n",
    "                        M = cv2.getRotationMatrix2D((rows / 2, cols / 2), theta1 + int(random.randrange(0,360,36)), 1.0)  # rotate counter clock-wise\n",
    "                        # M = np.float32([[1 - 0.1 * k, 0, 0], [0, 1 - 0.1 * k, 0]])     # zoom out\n",
    "                        # M = np.float32([[1 - 0.05 * k, 0, 0], [0, 1 - 0.05 * k, 0]])     # zoom out less aggressive\n",
    "                        dst = cv2.warpAffine(img0, M, (cols, rows))\n",
    "                        images2[j * 2, k, :, :] = torch.from_numpy(dst)\n",
    "                    labels2[j * 2] = labels[j]\n",
    "                for k in range(1, 11):\n",
    "                    if k == 0 or k == 9:\n",
    "                        images2[j * 2, k, :, :] = torch.from_numpy(img0)\n",
    "                    else:\n",
    "\n",
    "                        M = cv2.getRotationMatrix2D((rows / 2, cols / 2),theta2 - int(random.randrange(0,360,36)) * 36, 1.0)  # rotate clock-wise\n",
    "                        # M = np.float32([[0.1 * k, 0, 0], [0, 0.1 * k, 0]])    # zoom in\n",
    "                        # M = np.float32([[0.5 + 0.05 * k, 0, 0], [0, 0.5 + 0.05 * k, 0]])    # zoom in less aggressive\n",
    "                        dst = cv2.warpAffine(img0, M, (cols, rows))\n",
    "                        images2[j * 2 + 1, k - 1, :, :] = torch.from_numpy(dst)\n",
    "                    labels2[j * 2 + 1] = labels[j] + 10\n",
    "            inputs = images2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = snn(inputs)\n",
    "            labels_ = torch.zeros(batch_size * 2, 20).scatter_(1, labels2.view(-1, 1), 1)\n",
    "            loss = criterion(outputs.cpu(), labels_)\n",
    "            _, predicted = outputs.cpu().max(1)\n",
    "\n",
    "            # ----- showing confussion matrix -----\n",
    "\n",
    "            cm += confusion_matrix(labels2, predicted)\n",
    "            # ------ showing some of the predictions -----\n",
    "            # for image, label in zip(inputs, predicted):\n",
    "            #     for img0 in image.cpu().numpy():\n",
    "            #         cv2.imshow('image', img0)\n",
    "            #         cv2.waitKey(100)\n",
    "            #     print(label.cpu().numpy())\n",
    "\n",
    "            total += float(labels2.size(0))\n",
    "            correct += float(predicted.eq(labels2).sum().item())\n",
    "            if batch_idx % 100 == 0:\n",
    "                acc = 100. * float(correct) / float(total)\n",
    "                print(batch_idx, len(test_loader), ' Acc: %.5f' % acc)\n",
    "    class_names = ['0_ccw', '1_ccw', '2_ccw', '3_ccw', '4_ccw',\n",
    "             '5_ccw', '6_ccw', '7_ccw', '8_ccw', '9_ccw',\n",
    "             '0_cw', '1_cw', '2_cw', '3_cw', '4_cw',\n",
    "             '5_cw', '6_cw', '7_cw', '8_cw', '9_cw']\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "    print('Iters:', real_epoch, '\\n\\n\\n')\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.3f' % (100 * correct / total))\n",
    "    acc = 100. * float(correct) / float(total)\n",
    "    acc_record.append(acc)\n",
    "    if real_epoch % 5 == 0:\n",
    "        print(acc)\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': snn.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'acc_record': acc_record,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt' + names + '.t7')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}