{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "-> MNE is the main library for EEG signal processing in Python. It has almost all the functionality from EEGLab-Matlab and you can import EEGLab set files with a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data: <br>\n",
    "Load the calibration.set file using MNE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../BSPM/data/calibration.fdt\n",
      "Reading 0 ... 248153  =      0.000 ...   969.348 secs...\n"
     ]
    }
   ],
   "source": [
    "calibration_path = \"../BSPM/data/calibration.set\"\n",
    "eeg = mne.io.read_raw_eeglab(calibration_path,preload=True)\n",
    "data, times = eeg[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample the data: <br>\n",
    "In the next step we downsample the data by 8 factors in order to process the data more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawEEGLAB  |  calibration.fdt, n_channels x n_times : 27 x 31019 (969.3 sec), ~6.5 MB, data loaded>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg.resample(eeg.info['sfreq']/8) # Downsample the data by 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract epoches of the events. <br>\n",
    "What we want is to get the epochs starting from 0.2 secs after the event marker and ending at the 0.8 secs. <br>  However due to the MNE implementation it is not possible, which is why we start at 0.2 secs before the event.\n",
    "We epoch only the events labeled S4: No Error and S5: Error and extract the data for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['R  1', 'R  2', 'R  3', 'S  1', 'S  2', 'S  3', 'S  4', 'S  5', 'S  7', 'S  8', 'S  9', 'empty']\n",
      "190 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 190 events and 33 original time points ...\n",
      "0 bad epochs dropped\n",
      "110 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 110 events and 33 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "(events_from_annot,event_dict) = mne.events_from_annotations(eeg)\n",
    "epochs_noError = mne.Epochs(eeg,events_from_annot,tmin=-0.2,tmax=0.8,event_id=7,preload=True) # S 4\n",
    "epochs_Error = mne.Epochs(eeg,events_from_annot,tmin=-0.2,tmax=0.8,event_id=8,preload=True) # S 5\n",
    "noError_data = epochs_noError.get_data()\n",
    "error_data = epochs_Error.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually want the two points 210.9 and 285.2, unfortunately tmin cannot be larger or equal than 0.0 therefore we have to start epoching before the event. <br>\n",
    "<br>\n",
    "The epoching process tells us it gathered 33 events between -0.2 and 0.8. That is a time distance of 1. We need the time points at 0.2109 and 0.2852. <br>\n",
    "<br>\n",
    "We do a simple ratio calculation for both: <br>\n",
    "Timescale of 1.0 seconds is divided into 33 discreet time points <br>\n",
    "1.0/33 = 0.03 seconds of precision. <br>\n",
    "For the timepoint of 0.2109: <br> \n",
    "-0.2 + 0.03x = 0.2109 <br>\n",
    "0.03x = 0.4109 <br>\n",
    "x = 0.4109 / 0.03 = 13.69 ~ index 14 <br>\n",
    "For the timepoint of 0.2852: <br>\n",
    "-0.2 + 0.03x = 0.2852 <br>\n",
    "0.03x = 0.4852 <br>\n",
    "x = 0.4852/0.03 = 16.17 ~ index 16 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noError_data_210 = noError_data[:,:,14]\n",
    "noError_data_285 = noError_data[:,:,16]\n",
    "noError_data = np.concatenate((noError_data_210,noError_data_285), axis=1)\n",
    "error_data_210 = error_data[:,:,14]\n",
    "error_data_285 = error_data[:,:,16]\n",
    "error_data = np.concatenate((error_data_210,error_data_285), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data labels: <br>\n",
    "For no error use label 1 and for error events use label -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noError_label = np.ones(190)\n",
    "error_label = np.ones(110)*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the data and labels together: <br>\n",
    "Combine the data from no error and error epoches into a single 2D array. <br>\n",
    "Combine the labels into a single 1D array where the indexes match that of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((noError_data,error_data),axis=0)\n",
    "all_labels = np.concatenate((noError_label,error_label),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate discriminative power of the features using fisher rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "score = fisher_score.fisher_score(all_data, all_labels)\n",
    "rank = fisher_score.feature_ranking(score)\n",
    "score = np.flip(np.sort(score))\n",
    "x_axis = np.arange(score.shape[0])\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(x_axis,score)\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Fisher's score\")\n",
    "plt.title(\"Fisher's scores for components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first and second features using grouping. <br>\n",
    "As our features are super small, we have to normalize the data using minmax scaling. <br>\n",
    "In order to do a group scatter plot (gscatter in matlab), use a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7faf8ec706d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_feature = all_data[:,rank[0]]\n",
    "first_feature.reshape(1,-1)\n",
    "first_feature = (first_feature - first_feature.min()) / (first_feature.max() - first_feature.min())\n",
    "second_feature = all_data[:,rank[1]]\n",
    "second_feature.reshape(1,-1)\n",
    "second_feature = (second_feature - second_feature.min()) / (second_feature.max() - second_feature.min())\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\"First feature\": first_feature, \"Second feature\": second_feature, \"Category\": all_labels})\n",
    "groups = data.groupby(\"Category\")\n",
    "plt.figure()\n",
    "plt.xlabel(\"1st feature\")\n",
    "plt.ylabel(\"2nd feature\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"First feature\"], group[\"Second feature\"], marker=\"o\", linestyle=\"\", label=name, markersize=4)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the data columns into the normal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(all_data.shape[1]):\n",
    "    all_data[:,i] = (all_data[:,i] - all_data[:,i].min()) / (all_data[:,i].max() - all_data[:,i].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here split the data into pieces with K-fold K-times crossvalidation for K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X=all_data,y=all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier: (Shrink LDA with Shrinkage = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is: 0.9\n",
      "Score is: 0.9333333333333333\n",
      "Score is: 0.8333333333333334\n",
      "Score is: 0.8\n",
      "Score is: 0.8666666666666667\n",
      "Score is: 0.8\n",
      "Score is: 0.7333333333333333\n",
      "Score is: 0.9\n",
      "Score is: 0.8\n",
      "Score is: 0.6666666666666666\n",
      "Mean score is: 0.8233333333333335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "general_score = 0.0\n",
    "for train_index, test_index in kf.split(all_data):\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage=0.4)\n",
    "    clf.fit(all_data[train_index], all_labels[train_index])\n",
    "    score = clf.score(all_data[test_index], all_labels[test_index])\n",
    "    general_score += score\n",
    "    print(\"Score is: \" + str(score))\n",
    "print(\"Mean score is: \" +str(general_score/10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels on recall.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../BSPM/data/recall.fdt\n",
      "Reading 0 ... 268019  =      0.000 ...  1046.949 secs...\n",
      "Used Annotations descriptions: ['R  1', 'R  2', 'R  3', 'S  1', 'S  2', 'S  3', 'S  6', 'S  7', 'S  8', 'S  9', 'empty']\n",
      "{'R  1': 1, 'R  2': 2, 'R  3': 3, 'S  1': 4, 'S  2': 5, 'S  3': 6, 'S  6': 7, 'S  7': 8, 'S  8': 9, 'S  9': 10, 'empty': 11}\n",
      "300 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 300 events and 33 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "recall_path = \"../BSPM/data/recall.set\"\n",
    "eeg_recall = mne.io.read_raw_eeglab(recall_path,preload=True)\n",
    "data, times = eeg_recall[:, :]\n",
    "eeg_recall.resample(eeg_recall.info['sfreq']/8) # Downsample the data by 8.\n",
    "(events_from_annot,event_dict) = mne.events_from_annotations(eeg_recall)\n",
    "print(event_dict)\n",
    "#We require S 6: 7\n",
    "#7 is the event id with which we need to epoch from\n",
    "epochs_s6 = mne.Epochs(eeg_recall,events_from_annot,tmin=-0.2,tmax=0.8,event_id=7,preload=True, reject_by_annotation=False) # S 6\n",
    "epochs_data = epochs_s6.get_data()\n",
    "epochs_s6_210 = epochs_data[:,:,14]\n",
    "epochs_s6_285 = epochs_data[:,:,16]\n",
    "epochs_all_data = np.concatenate((epochs_s6_210,epochs_s6_285), axis=1)\n",
    "#Min Max Scaling on the epochs on recall.set\n",
    "for i in range(epochs_all_data.shape[1]):\n",
    "    epochs_all_data[:,i] = (epochs_all_data[:,i] - epochs_all_data[:,i].min()) / (epochs_all_data[:,i].max() - epochs_all_data[:,i].min())\n",
    "#Train a ShrinkLDA classifier on calibration.set\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage=0.4)\n",
    "clf.fit(all_data, all_labels)\n",
    "predicted = clf.predict(epochs_all_data)\n",
    "import scipy.io as sio\n",
    "sio.savemat('./lda.mat', {'vector_output': predicted})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
